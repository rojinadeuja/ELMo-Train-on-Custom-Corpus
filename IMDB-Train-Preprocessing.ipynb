{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data =  (25000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>32169</td>\n",
       "      <td>train</td>\n",
       "      <td>Contains spoilers The movie plot can be summar...</td>\n",
       "      <td>0</td>\n",
       "      <td>contains spoiler the movie plot can be summari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>29032</td>\n",
       "      <td>train</td>\n",
       "      <td>After being hugely entertained by Mr. Brosnan'...</td>\n",
       "      <td>0</td>\n",
       "      <td>after being hugely entertained by mr brosnan p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12919</th>\n",
       "      <td>37919</td>\n",
       "      <td>train</td>\n",
       "      <td>This film is an hour or so of good entertainme...</td>\n",
       "      <td>1</td>\n",
       "      <td>this film is an hour or so of good entertainme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18075</th>\n",
       "      <td>43075</td>\n",
       "      <td>train</td>\n",
       "      <td>Oh, come on, learn to have a little fun. When ...</td>\n",
       "      <td>1</td>\n",
       "      <td>oh come on learn to have little fun when wa ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>36821</td>\n",
       "      <td>train</td>\n",
       "      <td>The number of goofs in this episode was higher...</td>\n",
       "      <td>0</td>\n",
       "      <td>the number of goof in this episode wa higher t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7791</th>\n",
       "      <td>32791</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;\"step aside for hollywood veterans...</td>\n",
       "      <td>0</td>\n",
       "      <td>br br step aside for hollywood veteran the way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>38501</td>\n",
       "      <td>train</td>\n",
       "      <td>This is probably the best documentary I have s...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is probably the best documentary have see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>28386</td>\n",
       "      <td>train</td>\n",
       "      <td>Skippy from \"Family Ties\" plays Eddie, a wussy...</td>\n",
       "      <td>0</td>\n",
       "      <td>skippy from family tie play eddie wussy metal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>27455</td>\n",
       "      <td>train</td>\n",
       "      <td>She may have an Oscar and a Golden Globe, but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>she may have an oscar and golden globe but thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9810</th>\n",
       "      <td>34810</td>\n",
       "      <td>train</td>\n",
       "      <td>This was an awful movie. Basically Jane March ...</td>\n",
       "      <td>0</td>\n",
       "      <td>this wa an awful movie basically jane march wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type                                             review  \\\n",
       "7169        32169  train  Contains spoilers The movie plot can be summar...   \n",
       "4032        29032  train  After being hugely entertained by Mr. Brosnan'...   \n",
       "12919       37919  train  This film is an hour or so of good entertainme...   \n",
       "18075       43075  train  Oh, come on, learn to have a little fun. When ...   \n",
       "11821       36821  train  The number of goofs in this episode was higher...   \n",
       "7791        32791  train  <br /><br />\"step aside for hollywood veterans...   \n",
       "13501       38501  train  This is probably the best documentary I have s...   \n",
       "3386        28386  train  Skippy from \"Family Ties\" plays Eddie, a wussy...   \n",
       "2455        27455  train  She may have an Oscar and a Golden Globe, but ...   \n",
       "9810        34810  train  This was an awful movie. Basically Jane March ...   \n",
       "\n",
       "       sentiment                                  Processed_Reviews  \n",
       "7169           0  contains spoiler the movie plot can be summari...  \n",
       "4032           0  after being hugely entertained by mr brosnan p...  \n",
       "12919          1  this film is an hour or so of good entertainme...  \n",
       "18075          1  oh come on learn to have little fun when wa ki...  \n",
       "11821          0  the number of goof in this episode wa higher t...  \n",
       "7791           0  br br step aside for hollywood veteran the way...  \n",
       "13501          1  this is probably the best documentary have see...  \n",
       "3386           0  skippy from family tie play eddie wussy metal ...  \n",
       "2455           0  she may have an oscar and golden globe but thi...  \n",
       "9810           0  this wa an awful movie basically jane march wa...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Datasets/imdb_master_train.csv', encoding=\"latin-1\")\n",
    "print(\"Shape of training data = \", data.shape)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    story of man who ha unnatural feeling for pig ...\n",
       "1    airport 77 start a brand new luxury 747 plane ...\n",
       "2    this film lacked something couldn put my finge...\n",
       "3    sorry everyone know this is supposed to be an ...\n",
       "4    when wa little my parent took me along to the ...\n",
       "Name: Processed_Reviews, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding white space separated full stop to each sentence in data. There are 25K sentences in train.csv here.\n",
    "data_train['Processed_Reviews'] = data_train['Processed_Reviews'] + \" .\"\n",
    "data_train['Processed_Reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the training requires multiple files with one text sentence per line, \n",
    "# we will create 4K training files by writing 6 sentences per file. \n",
    "# After running the below python snippet, we get 4K files in train directory.\n",
    "\n",
    "if not os.path.exists(\"imdb/train\"):\n",
    "    os.makedirs(\"imdb/train\")\n",
    " \n",
    "for i in range(0,data_train.shape[0],6):\n",
    "    text = \"\\n\".join(data_train['Processed_Reviews'][i:i+6].tolist())\n",
    "    fp = open(\"imdb/train/\"+str(i)+\".txt\",\"w\")\n",
    "    fp.write(text)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Validation data is also prepared in the similar manner as training data.\n",
    "data_val['Processed_Reviews'] = data_val['Processed_Reviews'] + \" .\"\n",
    "if not os.path.exists(\"imdb/dev\"):\n",
    "    os.makedirs(\"imdb/dev\")\n",
    " \n",
    "for i in range(0,data_val.shape[0],6):\n",
    "    text = \"\\n\".join(data_val['Processed_Reviews'][i:i+6].tolist())\n",
    "    fp = open(\"imdb/dev/\"+str(i)+\".txt\",\"w\")\n",
    "    fp.write(text)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Vocabulary File\n",
    "The vocabulary file is a a text file with one token per line. It must also include the special tokens <S>, </S> and <UNK> (case sensitive) in the file. The vocabulary file should be sorted in descending order by token count in your training data. The first three lines should be the special tokens (<S>, </S> and <UNK>), then the most common token in the training data, ending with the least common token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in Training data =  4690217\n",
      "Size of Vocab 66146\n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join(data_train['Processed_Reviews'].tolist())\n",
    "words = texts.split(\" \")\n",
    "print(\"Number of tokens in Training data = \",len(words))\n",
    "dictionary = Counter(words)\n",
    "print(\"Size of Vocab\",len(dictionary))\n",
    "sorted_vocab = [\"<S>\",\"</S>\",\"<UNK>\"]\n",
    "sorted_vocab.extend([pair[0] for pair in dictionary.most_common()])\n",
    " \n",
    "text = \"\\n\".join(sorted_vocab)\n",
    "fp = open(\"imdb/vocab.txt\",\"w\")\n",
    "fp.write(text)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the biLM model\n",
    "We are ready to train our custom biLM model now.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python bin/train_elmo.py --train_prefix='imdb/train/*' --vocab_file 'imdb/vocab.txt' --save_dir 'imdb/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python bin/run_test.py --test_prefix='./imdb/dev/*' --vocab_file './imdb/vocab.txt' --save_dir './imdb/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python bin/dump_weights.py --save_dir 'imdb/checkpoint' --outfile 'imdb/imdb_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ds\n",
    "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    " \n",
    "# Location of pretrained LM.  Here we use the test fixtures.\n",
    "datadir = os.path.join('imdb', 'model')\n",
    "vocab_file = os.path.join(datadir, 'vocab.txt')\n",
    "options_file = os.path.join(datadir, 'options.json')\n",
    "weight_file = os.path.join(datadir, 'imdb_weights.hdf5')\n",
    " \n",
    "# Create a Batcher to map text to character ids.\n",
    "batcher = Batcher(vocab_file, 50)\n",
    " \n",
    "# Input placeholders to the biLM.\n",
    "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
    " \n",
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    " \n",
    "# Get ops to compute the LM embeddings.\n",
    "context_embeddings_op = bilm(context_character_ids)\n",
    " \n",
    "# Get an op to compute ELMo (weighted average of the internal biLM layers)\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    " \n",
    "# Now we can compute embeddings.\n",
    "raw_context = ['Technology has advanced so much in new scientific world',\n",
    "                'My child participated in fancy dress competition',\n",
    "                'Fashion industry has seen tremendous growth in new designs']\n",
    " \n",
    "tokenized_context = [sentence.split() for sentence in raw_context]\n",
    "print(tokenized_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # It is necessary to initialize variables once before running inference.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    # Create batches of data.\n",
    "    context_ids = batcher.batch_sentences(tokenized_context)\n",
    "    print(\"Shape of context ids = \", context_ids.shape)\n",
    " \n",
    "    # Compute ELMo representations (here for the input only, for simplicity).\n",
    "    elmo_context_input_ = sess.run(\n",
    "        elmo_context_input['weighted_op'],\n",
    "        feed_dict={context_character_ids: context_ids}\n",
    "    )\n",
    " \n",
    "print(\"Shape of generated embeddings = \",elmo_context_input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing euclidean distance between words embedding\n",
    "euc_dist_bet_tech_computer = np.linalg.norm(elmo_context_input_[1,5,:]-elmo_context_input_[0,0,:])\n",
    "euc_dist_bet_computer_fashion = np.linalg.norm(elmo_context_input_[1,5,:]-elmo_context_input_[2,0,:])\n",
    "# Computing cosine distance between words embedding\n",
    "cos_dist_bet_tech_computer = ds.cosine(elmo_context_input_[1,5,:],elmo_context_input_[0,0,:])\n",
    "cos_dist_bet_computer_fashion = ds.cosine(elmo_context_input_[1,5,:],elmo_context_input_[2,0,:])\n",
    " \n",
    "print(\"Euclidean Distance Comparison - \")\n",
    "print(\"\\nDress-Technology = \",np.round(euc_dist_bet_tech_computer,2),\"\\nDress-Fashion = \",\n",
    "      np.round(euc_dist_bet_computer_fashion,2))\n",
    "print(\"\\n\\nCosine Distance Comparison - \")\n",
    "print(\"\\nDress-Technology = \",np.round(cos_dist_bet_tech_computer,2),\"\\nDress-Fashion = \",\n",
    "      np.round(cos_dist_bet_computer_fashion,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
